import requests
import pandas as pd
from bs4 import BeautifulSoup
from lxml import html
# import urlparse
from urllib.parse import urljoin
def extractContent(url):
    try:
        return requests.get(str(url))
    except requests.exceptions.ConnectionError:
        pass
def getContent(url):
    response = extractContent(url)
    print(response.links)

# getContent('http://192.168.137.136/mutillidae/')
def extractHref_v1(url):
    try:
        page = requests.get(str(url))
        cont = html.fromstring(page.content)
        return cont.xpath('//a/@href')
    except requests.exceptions.ConnectionError:
        pass
# print(extractContent('http://192.168.137.136/mutillidae/'))
append_1 = []
def extractHref_v2(url):
    soup = BeautifulSoup(requests.get(str(url)).text, 'html.parser')
    for link in soup.find_all('a'):
        # print(link.get('href'))
        if 'http:' in link.get('href'):
            append_1.append({'links':link.get('href')})
    df = pd.DataFrame(append_1)
    # df.to_csv('metasploitable_mutillidae.csv')
    # df.to_html('metasploitable_mutillidae.html')
    print(df.head())
# extractHref_v2('http://192.168.137.136/mutillidae/')
append_2 = []
append_3 = []
def extractHttp_pandas():
    df = pd.read_csv('metasploitable_mutillidae.csv')
    for a,r in zip(list(df['links'][:3]), range(4)):
        responses = extractContent(str(a))
        append_2.append({'forms': BeautifulSoup(responses.content, 'html.parser').find_all("form")})
    df1 = pd.DataFrame(append_2)
    print(df1.head())
    for form in list(df['forms']):
        action = form.get("action")
        method = form.get("method")
        inputs = form.findAll("input")
        append_3.append({'forms':str(form),'action':str(action),'method':str(method),'inputs':str(inputs)})
    df2 = pd.DataFrame(append_3)
    print(df2.head())  
    df2.to_csv('forms-action.csv')
# extractHttp_pandas()
append_4 = []
def extractHttp_IO():
    df = pd.read_csv('metasploitable_mutillidae.csv')
    for a,r in zip(list(df['links']), range(4)):
        responses = extractContent(str(a))
        with open('forms_'+str(r)+'.html', 'a') as f:            
            f.write(str(BeautifulSoup(responses.content, 'html.parser').find_all("form")))
            action = f.write(str(BeautifulSoup(responses.content, 'html.parser').get("action")))
            # post_url = urlparse.urljoin(a, action)
            # post_url = urllib.urljoin(a, action)
            # with open('post_url.csv', 'a') as f:
            #     f.write(str(post_url))
            inputs = BeautifulSoup(responses.content, 'html.parser').findAll("input")
            for i in inputs:
                append_4.append({'name':i.get("name"),'type':i.get("type"),'value':i.get("value")})
    df1 = pd.DataFrame(append_4)
    df1.to_csv('name_type_value.csv')
    print(df1.head())
            

extractHttp_IO()

